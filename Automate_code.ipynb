{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6e358f1-b3a0-4f53-9763-8f0b6c83338f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a2baebc-bd33-4ad8-870d-003f259dbe80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: googletrans==4.0.0-rc1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.0.0rc1)\n",
      "Requirement already satisfied: httpx==0.13.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from googletrans==4.0.0-rc1) (0.13.3)\n",
      "Requirement already satisfied: certifi in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.8.30)\n",
      "Requirement already satisfied: hstspreload in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.12.1)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.0)\n",
      "Requirement already satisfied: chardet==3.* in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n",
      "Requirement already satisfied: idna==2.* in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.5.0)\n",
      "Requirement already satisfied: httpcore==0.9.* in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.1)\n",
      "Requirement already satisfied: h11<0.10,>=0.8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.0)\n",
      "Requirement already satisfied: h2==3.* in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.2.0)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (5.2.0)\n",
      "Requirement already satisfied: hpack<4,>=3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.0)\n",
      "Requirement already satisfied: googlesearch-python in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.2.5)\n",
      "Requirement already satisfied: beautifulsoup4>=4.9 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from googlesearch-python) (4.12.3)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from googlesearch-python) (2.32.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.9->googlesearch-python) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.20->googlesearch-python) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.20->googlesearch-python) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.20->googlesearch-python) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.20->googlesearch-python) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install googletrans==4.0.0-rc1\n",
    "!pip install googlesearch-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc55da95-a397-434d-a5d2-e0f6f85235d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea1fa8fb-8e8b-43b6-92ea-a558c0feca5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a8ec439-e6d3-4d26-9b2c-72af008dcb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from googletrans import Translator\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "import time\n",
    "import googlesearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40437903-ff16-48a1-9fb8-4c83de03244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Translator and Sentiment Analyzer\n",
    "translator = Translator()\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to scrape PIB website for headlines and links\n",
    "def scrape_pib_headlines():\n",
    "    url = \"https://pib.gov.in/allRel.aspx?reg=3&lang=1\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    headlines_links = []\n",
    "    \n",
    "    # Scrape headlines and links\n",
    "    for item in soup.find_all('div', class_='contentheading'):\n",
    "        headline = item.text.strip()\n",
    "        link = \"https://pib.gov.in/\" + item.find('a')['href']\n",
    "        headlines_links.append((headline, link))\n",
    "    \n",
    "    return headlines_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15cb5f1-347c-4e6c-8b94-ab367f33f60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform Google search for related articles\n",
    "def search_related_news(headline):\n",
    "    query = headline + \" site:in\"\n",
    "    search_results = googlesearch.search(query, num_results=5)\n",
    "    return search_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa308366-9393-4295-809d-83e21a0148d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to translate text to English\n",
    "def translate_to_english(text):\n",
    "    try:\n",
    "        translated = translator.translate(text, dest='en')\n",
    "        return translated.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error translating text: {e}\")\n",
    "        return text  # Return original text if translation fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde4a72c-f080-48cf-bf6d-0a739f02acd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform sentiment analysis\n",
    "def analyze_sentiment(text):\n",
    "    sentiment_score = sia.polarity_scores(text)['compound']\n",
    "    if sentiment_score >= 0.05:\n",
    "        return \"Positive\"\n",
    "    elif sentiment_score <= -0.05:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0bd91d-7c89-498f-8714-c2555ed72fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape and analyze individual news articles\n",
    "def scrape_and_analyze_article(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            paragraphs = [p.get_text(strip=True) for p in soup.find_all('p')]\n",
    "            text = ' '.join(paragraphs)\n",
    "            \n",
    "            if not text:\n",
    "                return None, None  # No text found\n",
    "            \n",
    "            # Translate the text to English if necessary\n",
    "            translated_text = translate_to_english(text)\n",
    "            sentiment = analyze_sentiment(translated_text)\n",
    "            \n",
    "            return sentiment, translated_text\n",
    "        else:\n",
    "            return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ffedf0-9b29-4d47-9f04-5f9df1dfd94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check for negative sentiment in related news\n",
    "def analyze_news_sentiment(headlines_links):\n",
    "    results = []\n",
    "    \n",
    "    for headline, pib_url in headlines_links:\n",
    "        print(f\"Analyzing PIB headline: {headline}\")\n",
    "        pib_sentiment = analyze_sentiment(headline)\n",
    "        \n",
    "        if pib_sentiment == \"Positive\":\n",
    "            # Search related news for the PIB headline\n",
    "            related_news = search_related_news(headline)\n",
    "            negative_news_found = False\n",
    "            \n",
    "            for news_url in related_news:\n",
    "                print(f\"Scraping related news: {news_url}\")\n",
    "                sentiment, translated_text = scrape_and_analyze_article(news_url)\n",
    "                \n",
    "                if sentiment == \"Negative\":\n",
    "                    # Highlight negative news for positive PIB news\n",
    "                    results.append([headline, pib_url, news_url, translated_text, sentiment])\n",
    "                    negative_news_found = True\n",
    "            \n",
    "            if not negative_news_found:\n",
    "                print(f\"No negative sentiment found for: {headline}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e05119-8134-4669-a75c-15e925def420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to store the results in a CSV file\n",
    "def store_results_in_csv(results, filename=\"negative_news.csv\"):\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['PIB Headline', 'PIB URL', 'Related News URL', 'Translated Text', 'Sentiment'])\n",
    "        \n",
    "        for result in results:\n",
    "            writer.writerow(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8cb24a-8e9a-4eab-b348-71baf9d69634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to automate the process\n",
    "def main():\n",
    "    while True:\n",
    "        print(\"Starting the automated news sentiment analysis system...\")\n",
    "\n",
    "        # Scrape PIB headlines and links\n",
    "        headlines_links = scrape_pib_headlines()\n",
    "        \n",
    "        # Analyze the news sentiment and gather results\n",
    "        results = analyze_news_sentiment(headlines_links)\n",
    "        \n",
    "        # Store the results in a CSV file\n",
    "        if results:\n",
    "            store_results_in_csv(results)\n",
    "            print(f\"Stored {len(results)} negative sentiment results.\")\n",
    "        else:\n",
    "            print(\"No negative sentiment results found.\")\n",
    "        \n",
    "        # Automate to run every 24 hours\n",
    "        print(\"Waiting for 24 hours before the next run...\")\n",
    "        time.sleep(86400)  # Wait for 24 hours (86400 seconds)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450a9aea-1d57-4694-b16e-1e93715373e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
