{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a0fa16b-6eae-46a0-b785-7d0d373b8799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ecedefe-77e8-492b-ac9e-41b8ddbe93d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: googletrans==4.0.0-rc1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.0.0rc1)\n",
      "Requirement already satisfied: httpx==0.13.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from googletrans==4.0.0-rc1) (0.13.3)\n",
      "Requirement already satisfied: certifi in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.8.30)\n",
      "Requirement already satisfied: hstspreload in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.12.1)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.0)\n",
      "Requirement already satisfied: chardet==3.* in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n",
      "Requirement already satisfied: idna==2.* in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.5.0)\n",
      "Requirement already satisfied: httpcore==0.9.* in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.1)\n",
      "Requirement already satisfied: h11<0.10,>=0.8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.0)\n",
      "Requirement already satisfied: h2==3.* in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.2.0)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (5.2.0)\n",
      "Requirement already satisfied: hpack<4,>=3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.0)\n",
      "Requirement already satisfied: googlesearch-python in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.2.5)\n",
      "Requirement already satisfied: beautifulsoup4>=4.9 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from googlesearch-python) (4.12.3)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from googlesearch-python) (2.32.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.9->googlesearch-python) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.20->googlesearch-python) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.20->googlesearch-python) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.20->googlesearch-python) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.20->googlesearch-python) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install googletrans==4.0.0-rc1\n",
    "!pip install googlesearch-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "061ec296-3227-457d-b938-3d5a6c111d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae6bc37d-7caf-43c9-adb7-78bd6f8ce98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd7dd882-3a62-49fd-ba7e-eff63cb50b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from googletrans import Translator\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "import time\n",
    "import googlesearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "395a480c-29e0-4a81-99b9-caf232aafab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape relevant data from a website\n",
    "def scrape_website(url, site_name):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Look for common content tags\n",
    "            candidates = soup.find_all(['article', 'section', 'div'])\n",
    "\n",
    "            # Select the candidate with the most text content\n",
    "            main_content = None\n",
    "            max_text_length = 0\n",
    "            \n",
    "            for candidate in candidates:\n",
    "                text_length = len(candidate.get_text(strip=True))\n",
    "                if text_length > max_text_length:\n",
    "                    max_text_length = text_length\n",
    "                    main_content = candidate\n",
    "\n",
    "            if not main_content:\n",
    "                print(f\"No relevant content found at {url}.\")\n",
    "                return None\n",
    "\n",
    "            # Extract paragraphs and headings from the selected content\n",
    "            paragraphs = [p.get_text(strip=True) for p in main_content.find_all('p')]\n",
    "            headings = [h.get_text(strip=True) for h in main_content.find_all(['h1', 'h2', 'h3'])]\n",
    "\n",
    "            # Filter out irrelevant content\n",
    "            relevant_paragraphs = [\n",
    "                p for p in paragraphs if len(p) > 30 and not any(keyword in p.lower() for keyword in [\"advertisement\", \"sponsored\", \"promo\"])\n",
    "            ]\n",
    "            relevant_headings = [\n",
    "                h for h in headings if len(h) > 5\n",
    "            ]\n",
    "\n",
    "            # Combine relevant headings and paragraphs for the relevant text\n",
    "            relevant_text = '\\n'.join(relevant_headings + relevant_paragraphs)\n",
    "\n",
    "            print(f\"Scraped relevant data from {url}\")\n",
    "            return relevant_text\n",
    "        else:\n",
    "            print(f\"Failed to retrieve {url}: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5666cd98-c1cc-42b0-9bbe-c6f6317b50cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to translate text to English\n",
    "def translate_to_english(text):\n",
    "    translator = Translator()\n",
    "    try:\n",
    "        translated = translator.translate(text, dest='en')\n",
    "        return translated.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error translating text: {e}\")\n",
    "        return text  # Return original text if translation fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b32c3d14-d859-4a70-8ece-ed80c942b20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform sentiment analysis using NLTK's VADER\n",
    "def analyze_sentiment(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment_score = sia.polarity_scores(text)['compound']\n",
    "    \n",
    "    # Determine sentiment category\n",
    "    if sentiment_score >= 0.05:\n",
    "        return \"Positive\"\n",
    "    elif sentiment_score <= -0.05:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5c3cdf2-4d8c-4527-aff3-5139d87f5208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to store scraped data in a CSV file\n",
    "def store_data_in_csv(data, filename=\"scraped_data.csv\"):\n",
    "    with open(filename, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f7163356-d74e-4c42-b71d-ac938b59959d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # List of websites to scrape\n",
    "    websites = [\n",
    "        {\"name\": \"Telugu Hindustan Times\", \"url\": \"https://telugu.hindustantimes.com/andhra-pradesh/bandi-sanjay-letter-to-chandrababu-in-tirumala-laddu-case-121726822820037.html\"},\n",
    "        {\"name\": \"Times of India\", \"url\": \"https://timesofindia.indiatimes.com/india/mea-reacts-to-misleading-reports-of-indian-weapons-in-ukraine/articleshow/113497264.cms\"}\n",
    "        # Add more websites as needed\n",
    "    ]\n",
    "\n",
    "# Create CSV file and write headers\n",
    "    with open('scraped_data.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Site Name', 'URL', 'Text', 'Timestamp', 'Sentiment'])\n",
    "\n",
    "    # Scrape each website and store the data in CSV\n",
    "    for site in websites:\n",
    "        data = scrape_website(site[\"url\"], site[\"name\"])\n",
    "        if data:\n",
    "            translated_data = translate_to_english(data)  # Translate data to English if needed\n",
    "            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            # Analyze sentiment\n",
    "            sentiment = analyze_sentiment(translated_data)\n",
    "            # Store site name, URL, text, timestamp, and sentiment\n",
    "            store_data_in_csv([site[\"name\"], site[\"url\"], translated_data, timestamp, sentiment])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5941c9f3-0bc3-4911-829c-b2d13a2c1123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped relevant data from https://telugu.hindustantimes.com/andhra-pradesh/bandi-sanjay-letter-to-chandrababu-in-tirumala-laddu-case-121726822820037.html\n",
      "Scraped relevant data from https://timesofindia.indiatimes.com/india/mea-reacts-to-misleading-reports-of-indian-weapons-in-ukraine/articleshow/113497264.cms\n",
      "Web scraping and sentiment analysis finished.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    print(\"Web scraping and sentiment analysis finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1f0f691d-6324-476c-bc77-0633fa2933ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped relevant data from https://telugu.hindustantimes.com/andhra-pradesh/bandi-sanjay-letter-to-chandrababu-in-tirumala-laddu-case-121726822820037.html\n",
      "Scraped relevant data from https://timesofindia.indiatimes.com/india/mea-reacts-to-misleading-reports-of-indian-weapons-in-ukraine/articleshow/113497264.cms\n",
      "Web scraping and sentiment analysis finished.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from googletrans import Translator\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "# Download VADER lexicon if not already done\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Function to scrape relevant data from a website\n",
    "def scrape_website(url, site_name):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Look for common content tags\n",
    "            candidates = soup.find_all(['article', 'section', 'div'])\n",
    "\n",
    "            # Select the candidate with the most text content\n",
    "            main_content = None\n",
    "            max_text_length = 0\n",
    "            \n",
    "            for candidate in candidates:\n",
    "                text_length = len(candidate.get_text(strip=True))\n",
    "                if text_length > max_text_length:\n",
    "                    max_text_length = text_length\n",
    "                    main_content = candidate\n",
    "\n",
    "            if not main_content:\n",
    "                print(f\"No relevant content found at {url}.\")\n",
    "                return None\n",
    "\n",
    "            # Extract paragraphs and headings from the selected content\n",
    "            paragraphs = [p.get_text(strip=True) for p in main_content.find_all('p')]\n",
    "            headings = [h.get_text(strip=True) for h in main_content.find_all(['h1', 'h2', 'h3'])]\n",
    "\n",
    "            # Filter out irrelevant content\n",
    "            relevant_paragraphs = [\n",
    "                p for p in paragraphs if len(p) > 30 and not any(keyword in p.lower() for keyword in [\"advertisement\", \"sponsored\", \"promo\"])\n",
    "            ]\n",
    "            relevant_headings = [\n",
    "                h for h in headings if len(h) > 5\n",
    "            ]\n",
    "\n",
    "            # Combine relevant headings and paragraphs for the relevant text\n",
    "            relevant_text = '\\n'.join(relevant_headings + relevant_paragraphs)\n",
    "\n",
    "            print(f\"Scraped relevant data from {url}\")\n",
    "            return relevant_text\n",
    "        else:\n",
    "            print(f\"Failed to retrieve {url}: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to translate text to English\n",
    "def translate_to_english(text):\n",
    "    translator = Translator()\n",
    "    try:\n",
    "        translated = translator.translate(text, dest='en')\n",
    "        return translated.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error translating text: {e}\")\n",
    "        return text  # Return original text if translation fails\n",
    "\n",
    "# Function to perform sentiment analysis using NLTK's VADER\n",
    "def analyze_sentiment(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment_score = sia.polarity_scores(text)['compound']\n",
    "    \n",
    "    # Determine sentiment category\n",
    "    if sentiment_score >= 0.05:\n",
    "        return \"Positive\"\n",
    "    elif sentiment_score <= -0.05:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "# Function to store scraped data in a CSV file\n",
    "def store_data_in_csv(data, filename=\"scraped_data.csv\"):\n",
    "    with open(filename, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(data)\n",
    "\n",
    "def main():\n",
    "    # List of websites to scrape\n",
    "    websites = [\n",
    "        {\"name\": \"Telugu Hindustan Times\", \"url\": \"https://telugu.hindustantimes.com/andhra-pradesh/bandi-sanjay-letter-to-chandrababu-in-tirumala-laddu-case-121726822820037.html\"},\n",
    "        {\"name\": \"Times of India\", \"url\": \"https://timesofindia.indiatimes.com/india/mea-reacts-to-misleading-reports-of-indian-weapons-in-ukraine/articleshow/113497264.cms\"}\n",
    "        # Add more websites as needed\n",
    "    ]\n",
    "    \n",
    "    # Create CSV file and write headers\n",
    "    with open('scraped_data.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Site Name', 'URL', 'Text', 'Timestamp', 'Sentiment'])\n",
    "\n",
    "    # Scrape each website and store the data in CSV\n",
    "    for site in websites:\n",
    "        data = scrape_website(site[\"url\"], site[\"name\"])\n",
    "        if data:\n",
    "            translated_data = translate_to_english(data)  # Translate data to English if needed\n",
    "            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            # Analyze sentiment\n",
    "            sentiment = analyze_sentiment(translated_data)\n",
    "            # Store site name, URL, text, timestamp, and sentiment\n",
    "            store_data_in_csv([site[\"name\"], site[\"url\"], translated_data, timestamp, sentiment])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    print(\"Web scraping and sentiment analysis finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bc19ed-511b-499f-a327-f69ca0015a54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
